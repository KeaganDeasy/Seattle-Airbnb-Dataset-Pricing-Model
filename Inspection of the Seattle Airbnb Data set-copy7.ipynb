{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/airbnb/seattle/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Business Questions - Brainstorming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " - Can we use this dataset to predict the price with a R^2 > 0.8?\n",
    "  - - am I able to understand the output of the LM or LASSO?\n",
    " - Does distance from the center of Seattle significantly influence the pricing model?\n",
    "  - - \n",
    " - Does our dataset contain characritics that are internally correlated to other characteristics\n",
    " - Identify pricing outliers\n",
    " \n",
    " occupancy rate Vs pricing effect?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#scikit learn packages\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LassoCV,RidgeCV\n",
    "#from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "#Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from sklearn.tree import export_graphviz\n",
    "#import pydot\n",
    "\n",
    "#Time functions\n",
    "#import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing=pd.read_csv(\"listings.csv\")\n",
    "#f_calendar=pd.read_csv(\"calendar\")\n",
    "#f_reviews="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#https://realpython.com/pandas-python-explore-dataset/\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for column in df_listing.columns:\n",
    "     print(\"\\n\" +\"\\n\"+\"\\n\"+ \"#######################   \" + column+\"   #######################\")\n",
    "     print(df_listing[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Useful column categorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "baseline_cols=['neighbourhood_group_cleansed','property_type','accommodates',\n",
    "       'price','number_of_reviews','review_scores_rating']\n",
    "\n",
    "#columns filled with Null data, uncleaned data\n",
    "nonvalue_cols=[\"experiences_offered\",\"neighbourhood\",\"neighbourhood_cleansed\",\"city\",\"state\",\"market\",\n",
    "               \"smart_location\",\"country_code\",\"country\",\"has_availability\",\"calendar_last_scraped\",\n",
    "               \"requires_license\",\"jurisdiction_names\",\"license\",\"host_acceptance_rate\",\"last_review\",\n",
    "               \"first_review\",\"calendar_updated\",\"host_neighbourhood\",\"zipcode\",\"bed_type\"]\n",
    "\n",
    "metadata_cols=[\"id\",\"listing_url\",\"scrape_id\",\"last_scraped\",\"thumbnail_url\",\"medium_url\",\"picture_url\",\n",
    "               \"xl_picture_url\",\"host_id\",\"host_url\",\"host_name\",\"host_since\",\"host_location\",\"host_thumbnail_url\",\n",
    "               \"host_picture_url\",\"host_total_listings_count\",\n",
    "               \"latitude\",\"longitude\"]\n",
    "\n",
    "concat_cols=[\"host_verifications\",\"amenities\"]\n",
    "\n",
    "textblock_cols=[\"name\",\"summary\",\"space\",\"description\",\"neighborhood_overview\",\"notes\",\"transit\",\"host_about\",\"street\",\"host_verifications\",\"amenities\"]\n",
    "\n",
    "dollar_cols=[\"cleaning_fee\",\"extra_people\",\"monthly_price\",\"price\",\"security_deposit\",\"weekly_price\"]\n",
    "\n",
    "bool_cols=[\"host_is_superhost\",\"host_has_profile_pic\",\"host_identity_verified\",\"instant_bookable\",\n",
    "\"is_location_exact\",\"require_guest_phone_verification\",\"require_guest_profile_picture\"]\n",
    "\n",
    "percentage_cols=[\"host_response_rate\"]\n",
    "\n",
    "multicolinearity_cols=[\"availability_60\",\"availability_90\",\"beds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Which columns are missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_nans(df,cut_off):\n",
    "    '''\n",
    "    INPUT : Dataframe\n",
    "            cut_off decimal to remove columns with less than this value of Nan\n",
    "    \n",
    "    OUTPUT: Graph with % of nan values in each column\n",
    "\n",
    "    '''\n",
    "    \n",
    "    global df_nan\n",
    "    df_nan = pd.DataFrame(df.isna().sum() / df.shape[0]).reset_index()\n",
    "    df_nan.rename(columns={'index':'column',0:'na_per'},inplace=True)\n",
    "    \n",
    "    # sort df by Count column\n",
    "    df_nan = df_nan.sort_values(['na_per'],ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Remove fll columns\n",
    "    df_nan=df_nan[df_nan[\"na_per\"]>cut_off]\n",
    "\n",
    "    base_color = sns.color_palette()[0]\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.title('Percentage of missing values by column')\n",
    "    sns.barplot(data=df_nan,y='column', x='na_per', color=base_color)\n",
    "    \n",
    "    return df_nan\n",
    "display_nans(df_listing,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_df(df,target,exclusion_list=[],dollar_cols=[],bool_cols=[],percentage_cols=[]):\n",
    "    \n",
    "        #Drop rows where the target has missing values\n",
    "        df  = df.dropna(subset=[target], axis=0)\n",
    "        \n",
    "        #drop exclusion list\n",
    "        df=df.drop(exclusion_list, axis=1)\n",
    "        \n",
    "        #expand concatenated columns\n",
    "        ######\n",
    "        \n",
    "        #\n",
    "        for col in dollar_cols:\n",
    "            try :df[col]=df[col].str.replace(\"[$, ]\", \"\")  \n",
    "            except: continue\n",
    "            df[col]=df[col].astype(\"float\")\n",
    "            \n",
    "        # text bool to bool type\n",
    "\n",
    "        dic={'t': True, 'f': False}\n",
    "        for col in bool_cols:\n",
    "            df[col]=df[col].replace(dic).astype(\"bool\")\n",
    "\n",
    "        \n",
    "        for col in percentage_cols:\n",
    "            df[col]=df[col].str.replace(\"[%, ]\", \"\")  \n",
    "            df[col]=df[col].astype(\"float\")\n",
    "            df[col] = df[col].div(100)\n",
    "        \n",
    "        display_nans(df,0.3)\n",
    "        nan_cols=df_nan[\"column\"]\n",
    "        df=df.drop(nan_cols,axis=1)\n",
    "        \n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        df_listing_num = df.select_dtypes(include=numerics)\n",
    "        # Mean function\n",
    "        fill_mean = lambda col: col.fillna(col.mean())\n",
    "        # Fill the mean\n",
    "        df_listing_num = df_listing_num.apply(fill_mean, axis=0)\n",
    "\n",
    "        #Pull a list of the column names of the categorical variables\n",
    "        cat_df = df.select_dtypes(include=['object'])\n",
    "        cat_cols = cat_df.columns\n",
    "\n",
    "        df_listing_cat=df.copy()\n",
    "\n",
    "        for col in  cat_cols:\n",
    "            df_listing_cat = pd.concat([df_listing_cat.drop(col, axis=1), pd.get_dummies(df_listing_cat[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=True)], axis=1)\n",
    "\n",
    "        # Mean function\n",
    "        fill_mean = lambda col: col.fillna(col.mean())\n",
    "        # Fill the mean\n",
    "        df_listing_cat = df_listing_cat.apply(fill_mean, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove non value added columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing=df_listing.drop(nonvalue_cols, axis=1)\n",
    "df_listing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Drop metadata columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There appears to be a number of characteristics that are unique to the webscraping activites. Additionally there are some unique idenfitifiers relating to the host (ID, urls ect)\n",
    "\n",
    "\n",
    "As recommended in many information sources, this is a likely cause of overfitting so I will preemtively remove these values\n",
    "\n",
    "https://stats.stackexchange.com/questions/224565/overfitting-due-to-a-unique-identifier-among-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing=df_listing.drop(metadata_cols, axis=1)\n",
    "df_listing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Drop text blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing=df_listing.drop(textblock_cols, axis=1)\n",
    "df_listing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Expand concatenated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "    \n",
    "https://stackoverflow.com/questions/28121682/quickest-way-to-make-a-get-dummies-type-dataframe-from-a-column-with-a-multiple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I would separare it into different elements\n",
    "And then have all the unique values as columbs\n",
    "And then do an IF formula\n",
    "That puts a 1 if it contains the word from the columb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def clean_to_text_dummies(df,col_list,characters,):\n",
    "    \n",
    "    # clean characters\n",
    "    # https://stackoverflow.com/questions/13682044/remove-unwanted-parts-from-strings-in-a-column\n",
    "    try :df[col]=df[col].str.replace(\"[$, ]\", \"\")\n",
    "    \n",
    "    https://www.rexegg.com/regex-quickstart.html\n",
    "    for col in col_list:\n",
    "    df_listing[col].str.get_dummies(sep=',')\n",
    "\n",
    "df['col2'].str.get_dummies(sep=',')\n",
    "pd.concat([df, df['col2'].str.get_dummies(sep=',')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Convert dollars to floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Inspection of the data shows that there are many currency/numeric columns that are captured as objects as they contain \"$\"\n",
    "\n",
    "Let's clean this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dollar_to_float(df,ls):\n",
    "    '''\n",
    "    INPUT : \n",
    "        dataframe\n",
    "        list of columns\n",
    "            \n",
    "    OUTPUT: dataframe with specified columns stripped of $ and spaces and returned as floats\n",
    "    '''\n",
    "    for col in ls:\n",
    "        try :df[col]=df[col].str.replace(\"[$, ]\", \"\")  \n",
    "        except: continue\n",
    "        df[col]=df[col].astype(\"float\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_of_obs=[]\n",
    "df_listing=dollar_to_float(df_listing,dollar_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Convert t/f to boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def str_to_bool(df,col_list,dic):\n",
    "    '''\n",
    "    Input\n",
    "    Output\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        df[col]=df[col].replace(dic).astype(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dic={'t': True, 'f': False}\n",
    "\n",
    "str_to_bool(df_listing,bool_cols,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Convert percentage strings to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perc_to_float(df,ls):\n",
    "    '''\n",
    "    INPUT : \n",
    "        dataframe\n",
    "        list of columns\n",
    "            \n",
    "    OUTPUT: dataframe with specified columns stripped of '%', converted to a float and made a decimal\n",
    "    '''\n",
    "    for col in ls:\n",
    "        df[col]=df[col].str.replace(\"[%, ]\", \"\")  \n",
    "        df[col]=df[col].astype(\"float\")\n",
    "        #df[col] = df[col].div(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\n",
    "def perc_to_float(df,ls):\n",
    "    '''\n",
    "    INPUT : \n",
    "        dataframe\n",
    "        list of columns\n",
    "            \n",
    "    OUTPUT: dataframe with specified columns stripped of $ and spaces and returned as floats\n",
    "    '''\n",
    "    for col in ls:\n",
    "        df[col] = df[col].str.rstrip('%').astype('float') /100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perc_to_float(df_listing,percentage_cols)\n",
    "df_listing.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cleaning high % Nan Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_nans(df_listing,0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we can see that almost 100% of the listing's area is missing. Although this information would be useful (based on experience) it makes sense to drop this as does the remaining values above\n",
    "\n",
    "I will remove these values and can readd them if Im not meeting my R^2 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nan_cols=[\"license\",\"square_feet\",\"monthly_price\",\"security_deposit\",\"weekly_price\",\"cleaning_fee\"]\n",
    "nan_cols=df_nan[\"column\"]\n",
    "df_listing=df_listing.drop(nan_cols,axis=1)\n",
    "df_listing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove observations where the target column is Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Dropping where the price has missing values\n",
    "df_listing  = df_listing.dropna(subset=['price'], axis=0)\n",
    "\n",
    "df_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fill mean to Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df_listing_num = df_listing.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Mean function\n",
    "fill_mean = lambda col: col.fillna(col.mean())\n",
    "# Fill the mean\n",
    "df_listing_num = df_listing_num.apply(fill_mean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### All Categorical objects to dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #Pull a list of the column names of the categorical variables\n",
    "    cat_df = df_listing.select_dtypes(include=['object'])\n",
    "    cat_cols = cat_df.columns\n",
    "    cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#dummy all the cat_cols\n",
    "df_listing_cat=df_listing.copy()\n",
    "\n",
    "for col in  cat_cols:\n",
    "    df_listing_cat = pd.concat([df_listing_cat.drop(col, axis=1), pd.get_dummies(df_listing_cat[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=True)], axis=1)\n",
    "\n",
    "df_listing_cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # Mean function\n",
    "    fill_mean = lambda col: col.fillna(col.mean())\n",
    "    # Fill the mean\n",
    "    df_listing_cat = df_listing_cat.apply(fill_mean, axis=0)\n",
    "\n",
    "    df_listing_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Baseline dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Dropping where the salary has missing values\n",
    "df_listing_base  = df_listing_cat.drop(baseline_cols, axis=1)\n",
    "\n",
    "df_listing_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model 1: Linear Regression - Model based on Numeric values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Split into explanatory and response variables\n",
    "X = df_listing_num.drop(\"price\", axis=1)\n",
    "y = df_listing_num[\"price\"]    \n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "#Predict using your model\n",
    "y_test_preds = lm_model.predict(X_test)\n",
    "y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "#Score using your model\n",
    "test_score = r2_score(y_test, y_test_preds)\n",
    "train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "#sklearn.metrics.mean_absolute_percentage_error(y_test, y_test_preds, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model 2: Linear Regression -  Model based on Select Numeric and categorical values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Split into explanatory and response variables\n",
    "X = df_listing_cat.drop(\"price\", axis=1)\n",
    "y = df_listing_cat[\"price\"]    \n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "#Predict using your model\n",
    "y_test_preds = lm_model.predict(X_test)\n",
    "y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "#Score using your model\n",
    "test_score = r2_score(y_test, y_test_preds)\n",
    "train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "#sklearn.metrics.mean_absolute_percentage_error(y_test, y_test_preds, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Linear Regression - Model based on Select Numeric and categorical values only with multicolinearity reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multicollinearity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generating pairwise correlation\n",
    "# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "corr = df_listing_num.corr()\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df_listing_num, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Displaying dataframe as an heatmap \n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_listing_cat_multico=df_listing_cat.drop(multicolinearity_cols,axis=1)\n",
    "df_listing_cat_multico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Split into explanatory and response variables\n",
    "X = df_listing_cat_multico.drop(\"price\", axis=1)\n",
    "y = df_listing_cat_multico[\"price\"]    \n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "#Predict using your model\n",
    "y_test_preds = lm_model.predict(X_test)\n",
    "y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "#Score using your model\n",
    "test_score = r2_score(y_test, y_test_preds)\n",
    "train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "#sklearn.metrics.mean_absolute_percentage_error(y_test, y_test_preds, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_lm_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "\n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "\n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "        lm_model = LinearRegression(normalize=True)\n",
    "        lm_model.fit(X_train, y_train)\n",
    "        y_test_preds = lm_model.predict(X_test)\n",
    "        y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "\n",
    "    best_cutoff = max(results, key=results.get)\n",
    "\n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "    #fit the model\n",
    "    lm_model = LinearRegression(normalize=True)\n",
    "    lm_model.fit(X_train, y_train)\n",
    "\n",
    "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r2_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r2_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cutoffs = [10000,5000, 3500, 2500, 1000, 100, 50, 30, 25,10,5]\n",
    "find_optimal_lm_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r2_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing=pd.read_csv(\"listings.csv\")\n",
    "#df_listing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nans(df,cut_off,display=True):\n",
    "    '''\n",
    "    INPUT : Dataframe\n",
    "            cut_off decimal to remove columns with less than this value of Nan\n",
    "    \n",
    "    OUTPUT: Graph with % of nan values in each column\n",
    "\n",
    "    '''\n",
    "    \n",
    "    global df_nan\n",
    "    df_nan = pd.DataFrame(df.isna().sum() / df.shape[0]).reset_index()\n",
    "    df_nan.rename(columns={'index':'column',0:'na_per'},inplace=True)\n",
    "    \n",
    "    # sort df by Count column\n",
    "    df_nan = df_nan.sort_values(['na_per'],ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Remove fll columns\n",
    "    df_nan=df_nan[df_nan[\"na_per\"]>cut_off]\n",
    "    \n",
    "    if display ==True:\n",
    "        base_color = sns.color_palette()[0]\n",
    "        plt.figure(figsize=(16,16))\n",
    "        plt.title('Percentage of missing values by column')\n",
    "        sns.barplot(data=df_nan,y='column', x='na_per', color=base_color)\n",
    "    \n",
    "    return df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df,target,exclusion_list=[],dollar_cols=[],bool_cols=[],percentage_cols=[]):\n",
    "    \n",
    "        global df_listing_num,df_listing_cat\n",
    "        #Drop rows where the target has missing values\n",
    "        df=df.dropna(subset=[target], axis=0)\n",
    "        \n",
    "        #drop exclusion list\n",
    "        df=df.drop(exclusion_list, axis=1,errors=\"ignore\")\n",
    "        \n",
    "        #expand concatenated columns\n",
    "        ######\n",
    "        \n",
    "        #\n",
    "        for col in dollar_cols:\n",
    "            try :df[col]=df[col].str.replace(\"[$, ]\", \"\")  \n",
    "            except: continue\n",
    "            df[col]=df[col].astype(\"float\")\n",
    "            \n",
    "        # text bool to bool type\n",
    "\n",
    "        dic={'t': True, 'f': False}\n",
    "        for col in bool_cols:\n",
    "            try:df[col]=df[col].replace(dic).astype(\"bool\")\n",
    "            except: continue\n",
    "        \n",
    "        for col in percentage_cols:\n",
    "            try:df[col]=df[col].str.replace(\"[%, ]\", \"\")  \n",
    "            except:continue\n",
    "            df[col]=df[col].astype(\"float\")\n",
    "            df[col] = df[col].div(100)\n",
    "        \n",
    "        display_nans(df,0.3,False)\n",
    "        nan_cols=df_nan[\"column\"]\n",
    "        df=df.drop(nan_cols,axis=1,errors=\"ignore\")\n",
    "        \n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        df_listing_num = df.select_dtypes(include=numerics)\n",
    "        # Mean function\n",
    "        fill_mean = lambda col: col.fillna(col.mean())\n",
    "        # Fill the mean\n",
    "        df_listing_num = df_listing_num.apply(fill_mean, axis=0)\n",
    "\n",
    "        #Pull a list of the column names of the categorical variables\n",
    "        cat_df = df.select_dtypes(include=['object'])\n",
    "        cat_cols = cat_df.columns\n",
    "\n",
    "        df_listing_cat=df.copy()\n",
    "\n",
    "        for col in  cat_cols:\n",
    "            df_listing_cat = pd.concat([df_listing_cat.drop(col, axis=1), pd.get_dummies(df_listing_cat[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=True)], axis=1)\n",
    "\n",
    "        # Mean function\n",
    "        fill_mean = lambda col: col.fillna(col.mean())\n",
    "        # Fill the mean\n",
    "        df_listing_cat = df_listing_cat.apply(fill_mean, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base line Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cols=['neighbourhood_group_cleansed','property_type','accommodates',\n",
    "       'price','number_of_reviews','review_scores_rating']\n",
    "\n",
    "exclusion_list=df_listing.columns.drop(baseline_cols)\n",
    "\n",
    "dollar_cols=[\"cleaning_fee\",\"extra_people\",\"monthly_price\",\"price\",\"security_deposit\",\"weekly_price\"]\n",
    "\n",
    "bool_cols=[\"host_is_superhost\",\"host_has_profile_pic\",\"host_identity_verified\",\"instant_bookable\",\n",
    "\"is_location_exact\",\"require_guest_phone_verification\",\"require_guest_profile_picture\"]\n",
    "\n",
    "percentage_cols=[\"host_response_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_base=df_listing.copy()\n",
    "clean_df(df_listing_base,\"price\",exclusion_list,dollar_cols,bool_cols,percentage_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into explanatory and response variables\n",
    "X = df_listing_cat.drop(\"price\", axis=1)\n",
    "y = df_listing_cat[\"price\"]    \n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "#Predict using your model\n",
    "y_test_preds = lm_model.predict(X_test)\n",
    "y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "#Score using your model\n",
    "test_score = r2_score(y_test, y_test_preds)\n",
    "train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "print(\"test score:\",test_score,\" train score:\",train_score)\n",
    "#sklearn.metrics.mean_absolute_percentage_error(y_test, y_test_preds, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list=[\"experiences_offered\",\"neighbourhood\",\"neighbourhood_cleansed\",\"city\",\"state\",\"market\",\n",
    "                \"smart_location\",\"country_code\",\"country\",\"has_availability\",\"calendar_last_scraped\",\n",
    "                \"requires_license\",\"jurisdiction_names\",\"license\",\"host_acceptance_rate\",\"last_review\",\n",
    "                \"first_review\",\"calendar_updated\",\"host_neighbourhood\",\"zipcode\",\"bed_type\",\"id\",\n",
    "                \"listing_url\",\"scrape_id\",\"last_scraped\",\"thumbnail_url\",\"medium_url\",\"picture_url\",\n",
    "                \"xl_picture_url\",\"host_id\",\"host_url\",\"host_name\",\"host_since\",\"host_location\",\n",
    "                \"host_thumbnail_url\",\"host_picture_url\",\"host_total_listings_count\",\"latitude\",\n",
    "                \"longitude\",\"name\",\"summary\",\"space\",\"description\",\"neighborhood_overview\",\"notes\",\n",
    "                \"transit\",\"host_about\",\"street\",\"host_verifications\",\"amenities\",\"host_response_rate\",\n",
    "                \"host_is_superhost\",\"host_listings_count\",\"host_has_profile_pic\",\"host_identity_verified\",\n",
    "                \"is_location_exact\",\"minimum_nights\",\"maximum_nights\"]\n",
    "\n",
    "dollar_cols=[\"cleaning_fee\",\"extra_people\",\"monthly_price\",\"price\",\"security_deposit\",\"weekly_price\"]\n",
    "\n",
    "bool_cols=[\"host_is_superhost\",\"host_has_profile_pic\",\"host_identity_verified\",\"instant_bookable\",\n",
    "\"is_location_exact\",\"require_guest_phone_verification\",\"require_guest_profile_picture\"]\n",
    "\n",
    "percentage_cols=[\"host_response_rate\"]\n",
    "\n",
    "#df_listing,\"price\",exclusion_list=[],dollar_cols=[],bool_cols=[],percentage_cols=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_original=df_listing.copy()\n",
    "clean_df(df_listing_original,\"price\",exclusion_list,dollar_cols,bool_cols,percentage_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orr = df_listing_original.corr()\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    global au_corr\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df_listing_num, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features=[\"availability_90\",\"availability_60\",\"beds\"]\n",
    "\n",
    "df_listing_cat=df_listing_cat.drop(corr_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into explanatory and response variables\n",
    "X = df_listing_cat.drop(\"price\", axis=1)\n",
    "y = df_listing_cat[\"price\"]    \n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "#Predict using your model\n",
    "y_test_preds = lm_model.predict(X_test)\n",
    "y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "#Score using your model\n",
    "test_score = r2_score(y_test, y_test_preds)\n",
    "train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "print(\"test score:\",test_score,\" train score:\",train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train RF regressor model\n",
    "forest = RandomForestRegressor(n_estimators=900, \n",
    "                               criterion='mse', \n",
    "                               random_state=42, \n",
    "                               n_jobs=-1,verbose=1)\n",
    "forest.fit(X_train, y_train.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = forest.predict(X_train)\n",
    "y_test_preds = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_preds),\n",
    "        r2_score(y_test, y_test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
